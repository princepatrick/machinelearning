Research Paper: https://www.sciencedirect.com/science/article/pii/S000145752031770X?casa_token=mp7WXeVSu-YAAAAA:9ehsr0ifYMpYop3CYXujYYk1xGOLu489K-vrtj7CFsNTKE7oOV_pPhCj7cYYh2b9NEPGfcVMPU0

Crash data augmentation using variational autoencoder
	In this paper, millions of crash data was generated from a dataset that contatined minimal amount of crash data.
	THe original dataset consists of 625 of crash events and 6.5 million non-crash events.
	The dataset was put into a latent space using Variational Autoencoder, where both the crash and non-crash events was put into the latent space.
	The model then uses only the crash events in the latent space to generate the synthetic crash data.
	The generated data was then compared with real data in various aspects like t-test, Levene test and Kolmogrove Smirnov test.
	It was also compared to some oversampling techniques like SMOTE and ADASYN, using ML techniques like Support Vector Machine SVM, Logistic Regression LR and Artificial Neural Network ANN and other generational networkds like GAN and the performance was evaluated.
	Specifity is imporved by 8% and 4% in VAE-LR and VAE-SM when compared to SMOTE and the sensitivity by 6% and 5% when compared to ADASYN.

Introduction:
	The real-time crash likelihood prediction is vast and critical area of research.
	We are able to collect data from roads by the sensors we have in the roadways but one problem is that the crash and non-crash data is highly disproportionate- 1:11000
	Crashes are incidents that happen in sudden instances so we might need to deal with high turbulence data and based on the insights we can better plan the environment to avoid such incidents.
	Better planning could be of various type like demographic insights, engineering insights like lighting of the roadway and engineering of the road, control solutions like speed limits and ramp metering.
	There has been machine learning models that were built but the models have been biased towards non-crash data. This requirees more data for crash events.
	We can generate crash events through oversampling methods but they suffer from overfitting as we cannot set the decision boundaries.

Variational Autoencoders:
	An autoencoder consists of 3 parts that are linked to each other - Encoder, Latent Space and Decoder.
	THe encoder takes the input data and sends it to the latent space as a distribution and the decoder takes up the representation in the latent space to produce the data in the original format.
	The Variational Autoencoder was introduced in 2014 to handle the problems in AE to be a generative model. 
	The changes were done in the loss term or function and the encoded layers of the autoencoder

Dataset:
	The paper uses the data that was generated from the Microwave vehicle detection system(MVDS) from expressway SR 408 in Orlando.
	It is a 21.4 miles long road with 110 MVDS sensors a total of 417000 vehicles ride through it everyday.
	There were 24 features that is centralized around speed, volume and lane occupancy.
	THere were 625 crash events and 6.7 million non crash events, the 5-10 min window before and after the crash is labelled as crash data.

Approach:
	The VAE have been particularly used to a great extent in Computer Vision tasks that involve image oriented techniques.
	The image data is a collection of pixel values while the data collected from different sensors are also a series of values.
	x -> 2 encoders (24 neurons) -> latent space -> 2 decoders (24 neurons) -> x'
	70% of the real data was used for training.
	The batch size was divided into 437 crash-data and 437 non-crash data. The same crash-data was used for all the batches otherwise the model is biased to the non-crash data part.
	It was trained for 12000 epochs to consider all non-crash data.
	We have used a confidence ellipsoid to draw the latent space boundary

	Real Data: - 70% training, 30% test
	Undersampled data, ADASYN Data, SMOTE Data, VAE Data A, VAE Data B, VAE Data C
	This fed into LR, SVM and ANN
	Evaluating the trained model based on confusion matrix, specificity, sensitivity and AUC scores

There are in total 5 different datasets:
	1. Real Data - The original data from the datasets
	2. Undersampled Data - It contains all the crash data and a sample of non-crash data. The data is perfectly balanced - ie the number of crash and non-crash data are the same.
	3. ADASYN Data - An oversampling technique which also produces balanced data - the number of crash data is same as the number of non-crash data.
	4. SMOTE data - An oversampling technique which produces balanced data
	5. VAE Data(a, b, and c) - All these contains the generated data from the VAE and inaddition to that the 437 crash data. This is 3 sets of the VAE generated data. The 3 sets of the data is dependent on the confidence ellipsoid score of the data set.

	They are now being fed into classification tasks like Logistic Regression and Support Vector Machine and are being evaluated basd on specificity, sensitivity and AUC 

Specificity:
	It is the ability of the model to predict a non-crash data as a non-crash data
	Specificity = True Negative / True Negative + True Positive
	
Sensitivity:
	It is the ability of the model to predict the crash data as crash data
	Sensitivity = True Positive / True Negative + True Positive

AUC Score:
	The Area under Curve score. This is used to indicate how well the model is able to distinguish between the various values.

	VAE Data C - Confidence ellipsoid 90% is the model with the highest scores for the model among the VAE - Data(a, b,c) sets. THus VAE Data c set will be the representative model for VAE.
	
	Comparing VAE Data C with SMOTE and ADASYN:
		On comparing the operations for Logistic Regression, Support Vector Machine and Artificial Neural Network for SMOTE, ADASYN and VAE Data C, we find that for Logistic Regression SMOTE and ADASYN were better than VAE Data C and for the remaining two models of SVM and ANN, VAE Data C is proven to be better.
	
	Comparing VAE Data C with DCGAN:
		When compared with DCGAN, for LR and SVM VAE Data C proved to be the better model/data and when evaluated for ANN, AUC for DCGAN is 1% better while the sensitivity of VAE Data C is 4% better.

Statistical Comparison between Real and Synthetic Data:
	The study used t-test, Levene Test and Kolmogrove-Smirnov test for comparing real and synthetic data and the scores(p-value) were above 0.05 for each category which indicated high similarity between the datasets.