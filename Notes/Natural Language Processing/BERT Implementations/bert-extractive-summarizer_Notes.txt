what is bert-extractive-summarizer?
	It is a model that uses the pre-trained model from google by default, without any fine-tuning process done.
	It is a tool that uses HuggingFace Pytorch transformers library to run extractive summarization 
	Process:
		Embedd the sentence
		Run a clustering algorithm
		find the sentences that are closest to the centroid
		
	Note: It also uses the HuggingFace NeuralCoref library to resolve the words in the summary that needs context.

Process:
	Tokenized the Incoming paragraph into clean sentence
	The tokenized words are sent to BERT model to produce output embeddings
	The embeddings are clustered by KMeans
	Select the embeddings that are closest to the centroid


Tokenization:
	For transcripts retrieved from Udacity, a custom parser was created to convert the .srt file into a paragraph
	NLTK library is used to extract the sentences, breaking up the content to be fed into the models for inference
	Removing or editing sentences that needs additional context
	Removed the content relevant to Udacity quizzes.
	Remove small and large sentences
	
BERT for text embedding:
	The second to last averaged layer produced the best embeddings for representations of words
	One hypothesis for this is the final layer is biased by the classification tasks
	The BERT implementations uses the BERT model from the huggingface transformers library
	The BERT model in the huggingface transformer library is a python wrapper of the BERT model from Google and in addition to that also contains the GPT2 model.
	The ensembling of BERT and GPT2 models produced the best results for embeddings but lacked in the memory and speed efficiency.

Clustering Embeddings:
	TODO: When using KMeans - what is the K value used and how to find it
	Elbow method: Read Below - We find the number of sentences and send it as the maximum number of clusters that could be considered. The Elbow runs the elbow algorithm with each cluster number and then choose the point where the inertia does not reduce.
	
	Once the N-2 layer embeddings was completed, the N*E matrix was ready for clustering.
	The input for the number of clusters and sentences was provided by the user of the API
	Both KMeans and Gaussian models wre considered fro clustering but finally KMeans was selected, although the performance was similar.

Results:
	TODO: Metrics for Extractive Summarization
	Since there was no golden truth summaries for the data, the human interaction is the only metric that was used.
	
	Model Weakness:
		1. Handle Large text to be summarized into few sentences which led to losing context. One solution was to use multiple sentences that were closest to the centroid, but that would mean to go against the ratio parameter set by the user. Hence, would impact the user experience.
		2. Another weakness was the model included sentences that contain wordss like 'this', 'that' etc which lacked context on what it was referring to. A naive brute force approach would be to remove the sentences that contained such a word but this would impact the quality of the model. Another approach was to replace such words with the actual words they referred to but it was hard to do as sometimes the words were present in 2-3 sentences prior which made it hard to locate the reference exactly.

Further Improvements:
	The model has used the pre-trained Google BERT model by default, instead we could additionally fine tune the model by the Udacity Data so that the summarization could be better.
	Fill in the gaps for the missing context in the sentence
	The model itself predicts the right number of sentences needed to convey the summary of the paragraph. This could be done using the sum of squares method
	

trained, supervised, dataset, What metric and how the metric is evaluating the model?

Training: Only the pre-trained BERT model was directly used as default and no additional fine-tuning is done
Dataset: We dont have a training dataset but we only use the Udacity dataset for testing
Metric: No metric available so we had to use human evaluation

What is the use of using distilbert-base-uncased
By default bert-large-uncased (BERT Large) is used as the model, which is the large version of bert (340M parameters) but for the implementation we are using distilbert-base-uncased (Distil BERT) is using 44M parameters while preserving 90% of the performance of it.


Sentence-BERT: https://arxiv.org/pdf/1908.10084.pdf
	The BERT and ROBERTA model has become the state-of-the-art in various text related tasks.
	Though for sentence-pair regression tasks, BERT is heavily intensive and could need a lot of time. For eg: A BERT model planning to find the most similar sentence-pair in a list of 10000 sentences would require 50 million computations and 65 hours of computing time.
	This is the reason the model Sentence-BERT is introduced which uses Siamese and triplet networks to find the similarity between two sentences and this reduces the time needed to 5 seconds.
	
	Introduction
		Since the semantic search is a time consuming process, another Commonly Used method is to feed each sentence to the BERT model and then map the response in a vector space and the sentence that is closest to each other is used as the output but this process is in poor quality and leads to bad performance.
	
		In the SBERT , a siamese network on the dataset produces fixed-size vectors and by using a similarity measure like cosine similarity or Manhattan/Euclidean  distance, we can find the semantically similar sentences.
		The SBERT model is fine-tuned on NLI data.
		It out-performs other state-of-the-art sentence embedding methods like InferSent and Universal Sentence Encoder.
		
		It sets the state of the art for a challenging argument similarity dataset and a triple dataset to differentiate sentences from differnet sections of wikipedia.
	
	Related Work:
		BERT - state of the art results in various tasks on textual data
		RoBERTa - improvement on BERT with change in the pre-training process
		XLNet - worse than BERT
		A large disadvantage of the BERT model is that individual sentence embeddings are not formed as a result researcher use one of the 2 methods:
			Send sentence by sentence in the BERT model and then take the mean of the embeddings 
			Send the sentences to the BERT model and then use the embeddings that come out in the CLS token place.
		But the evaluation is not available to understand if they provided any improvement
	
	The Summarizer model could use Sentence-BERT instead of BERT for modelling the summarizer using a customized summarizer class called SBertSummarizer.
	The SBertSummarizer by default accepts all-mpnet-base-v2 but we can provide customized models like paraphrase-MiniLM-L6-v2

	Difference between all-mpnet-base-v2 and paraphrase-MiniLM-L6-v2:
		Architecture:
			all-mpnet-base-v2
				It is  based on an architecture called MPNet ie. Multi Lingual Parallel Corpus Network.
				The pre-training is done in multiple language to provide a cross-linguistic representations
			paraphrase-MiniLM-L6-v2:
				It is a lightweight version of the LM (Language Model)
				It is trained for paraphrase and semantic similarity tasks
				L6 refers to the number of layers in the architecture
		
		Trainig Data:
			all-mpnet-base-v2:
				It is multi-lingual in nature
			paraphrase-MiniLM-L6-v2:
				Layers: 6
				The dataset is specifically curated for paraphrasing and semantic similarity
				The dataset contains various pairs of sentences with the results as the similarity scores
				Training Data: AllNLI, sentence-compression, SimpleWiki, altlex, msmarco-triplets, quora_duplicates, coco_captions,flickr30k_captions, yahoo_answers_title_question, S2ORC_citation_pairs, stackexchange_duplicate_questions, wiki-atomic-edits
				Max Sequence Length: 128
				Dimensions: 384
				Normalized Embeddings: false
				Suitable Score Functions: cosine-similarity (util.cos_sim)
				Size: 80MB
				Pooling: Mean Pooling

				
			paraphrase-MiniLM-L3-v2:
				Layers: 3
				Training Data:	AllNLI, sentence-compression, SimpleWiki, altlex, msmarco-triplets, quora_duplicates, coco_captions,flickr30k_captions, yahoo_answers_title_question, S2ORC_citation_pairs, stackexchange_duplicate_questions, wiki-atomic-edits
		
	
	paraphrase-MiniLM-L6-v2:
		It is a lightweight version of the LM (Language Model)
		It is trained for paraphrase and semantic similarity tasks
		L6 refers to the number of layers in the architecture
		Layers: 6
		The dataset is specifically curated for paraphrasing and semantic similarity
		The dataset contains various pairs of sentences with the results as the similarity scores
		Training Data: AllNLI, sentence-compression, SimpleWiki, altlex, msmarco-triplets, quora_duplicates, coco_captions,flickr30k_captions, yahoo_answers_title_question, S2ORC_citation_pairs, stackexchange_duplicate_questions, wiki-atomic-edits
		Max Sequence Length: 128
		Dimensions: 384
		Normalized Embeddings: false
		Suitable Score Functions: cosine-similarity (util.cos_sim)
		Size: 80MB
		Pooling: Mean Pooling
	
		Evaluating for paraphrase:
			classsentence_transformers.evaluation.ParaphraseMiningEvaluator(sentences_map: Dict[str, str], duplicates_list: Optional[List[Tuple[str, str]]] = None, duplicates_dict: Optional[Dict[str, Dict[str, bool]]] = None, add_transitive_closure: bool = False, query_chunk_size: int = 5000, corpus_chunk_size: int = 100000, max_pairs: int = 500000, top_k: int = 100, show_progress_bar: bool = False, batch_size: int = 16, name: str = '', write_csv: bool = True)
			Given a large set of sentences, this evaluator performs paraphrase (duplicate) mining and identifies the pairs with the highest similarity. It compare the extracted paraphrase pairs with a set of gold labels and computes the F1 score.

SciBert:
SciBert addresses the lack of models to handle scientific tasks and the model is trained on 1.1 million scientific papers.
Every other architectural values are same with the BERT model

Architecture:
	The architecture is BERT

Vocabulary:
	The vocabulary of base bert is BaseVocab
	The vocabulary of science bert is SciVocab
	The Vocabulary size of SciVocab is 30k and it matches with the BaseVocab
	The token matching between BaseVocab and SciVocab is 42%

Corpus:
	Random sample of 1.14m papers in Semantic Scholar
	18% from CS domain, 82% from Biomedical domain
	154 sentences per paper and 2617 tokens per paper
	3.1B tokens in total and in comparison 3.3B in BERT

Training Method:
	Masked Language Modeling and Next Sentence Prediction
	Self-supervised learning

Evaluation Metrics:
	Not provided
	

Elbow Method:
	When working with the KMeans algorithm, we need to find the optimal number of clusters required to define the model
	Elbow method is one of the ways to find the optimal cluster limit.
	When the clusters are low in number, the data points are in general more close to each other and when the number of clusters increases, the data points get more sparse.
	The Elbow method revolves around calculating the sum of the squares of the distance between the data points. This is called the inertia.
	we will find after a particular number of cluster (the optimal point) we find even if we increase the number of clusters, the distance between the points do not increase at all.
	On plotting this sum of the squared distances(inertia) with the number of clusters, we will find the graph line will decreases until we reach the optimal point and then it remains constant throughout. This shape looks like an Elbow.

Evaluation Method:
	Not provided for bert-extractive-summarizer as there was no reference summaries
	Still ROUGE is the most used one for extractive summarization
	
	Abstractive Summary: The boy was reading his book in silence.
	Reference Summary: The boy is enjoying in silence.
	
	ROUGE-2:
		Bigrams in Abstractive Generated Summary: "The boy", "boy was", "was reading", "reading his", "his book", "book in", "in silence"
		Bigrams in Reference Summary: "The boy" , "boy is" , "is enjoying", "enjoying in" , "in silence"
		
		Common Bigrams: "The boy", "in silence" -> 2
		
		Number of bigrams in generated summary: 7
		Number of bigrams in reference summary: 5
		
		ROUGE-2 precision: 2/7 = 0.28571428
		ROUGE-2 Recall: 2/5 = 0.4
		
		F1-score: 2 * (precision*recall) /  (precision+recall) = 0.232 / 0.69 = 0.33

Steps to follow for 1 page (about 60 sentences)
	1. Run a extractive summary from 1 page to 
		20 sentences
		10 sentences
		5 sentences
		3 sentences
		2 sentences
		1 sentence
	using 
		BERT
		distil-bert
		SBERT
All done
		
